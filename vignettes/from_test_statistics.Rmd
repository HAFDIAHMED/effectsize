---
title: "Effect Size from Test Statistics"
output: 
  github_document:
    toc: true
    fig_width: 10.08
    fig_height: 6
  rmarkdown::html_vignette:
    toc: true
    fig_width: 10.08
    fig_height: 6
tags: [r, effect size, standardization, effect size, cohen d, standardized coefficients]
vignette: >
  %\VignetteIndexEntry{Effect Size from Test Statistics}
  \usepackage[utf8]{inputenc}
  %\VignetteEngine{knitr::rmarkdown}
editor_options: 
  chunk_output_type: console
bibliography: bibliography.bib
---

```{r message=FALSE, warning=FALSE, include=FALSE}
library(knitr)
library(effectsize)

knitr::opts_chunk$set(comment = ">")
options(digits = 2)
options(knitr.kable.NA = '')

if (!requireNamespace("afex", quietly = TRUE) ||
    !requireNamespace("lmerTest", quietly = TRUE) ||
    !requireNamespace("emmeans", quietly = TRUE) ||
    !requireNamespace("parameters", quietly = TRUE)
    ) {
  knitr::opts_chunk$set(eval = FALSE)
} else {
  library(afex)
  library(lmerTest)
  library(emmeans)
  library(parameters)
}

set.seed(747)
```

# Introduction

In many real world applications there are no straightforward way methods of obtaining standardized effect sizes. However, it is possible to get approximations of most of the indices ($d$, $r$, $\eta^2_p$...) by converting test statistics. These conversions are based on the idea that **test statistics are a function of effect size and sample size** (or degrees of freedom), and thus information about the samples size, is used to reverse engineer indices of effect size from test statistics.

All the measures discussed here are, in one way or another, ***signal to noise ratios***, with the noise being a property of the unaccounted variance in outcome variable^[Note that for generalized linear models (Poisson, Logistic...), where the outcome is never on a arbitrary scale, estimates themselves **are** indices of effect size! Thus this vignette is relevant only to general linear models.].

## (Partial) Percent Variance Explained

These measures represent the ratio of $Signal^2 / (Signal^2 + Noise^2)$, with the "noise" having all other "signals" partial-ed out (be they of other fixed or random effects). The conversion of the $F$- or $t$-statistic is based on @friedman1982simplified.

Let's look at an example:

```{r}
library(afex)

data(md_12.1)

aov_fit <- aov_car(rt ~ angle * noise + Error(id/(angle * noise)),
                   data = md_12.1,
                   anova_table=list(correction = "none", es = "pes"))
aov_fit
```

Let's compare the $\eta^2_p$ (`pes` column) obtained here with ones recovered from `F_to_eta2()`:

```{r}
library(effectsize)

F_to_eta2(
  f = c(40.72, 33.77, 45.31),
  df = c(2, 1, 2),
  df_error = c(18, 9, 18)
)
```

**They are identical!**^[Note that these are *partial* percent variance explained, and so their sum can be larger than 1. ] (except for the fact that `F_to_eta2()` also provides confidence intervals^[Confidence intervals are estimated using the Noncentrality parameter method; These methods searches for a the best non-central parameter for of the noncentral $F$/$t$ distribution for the desired tail-probabilities, and then convert these ncps to the corresponding effect sizes.] :)

In this case we were able to easily obtain the effect size, but in other cases it is not as easy, and using the conversion from test statistic is a good approximation...

### Effect sizes for Simple Effect

```{r}
library(emmeans)

joint_tests(aov_fit, by = "noise")

F_to_eta2(
  f = c(5, 79),
  df = 2,
  df_error = 29
)
```

### Effect sizes in Linear Mixed Models

```{r}
library(lmerTest)

fit_lmm <- lmer(Reaction ~ Days + (Days | Subject), sleepstudy)

anova(fit_lmm)

F_to_eta2(45.8, 1, 17)
```

We can also use `t_to_eta2()` (which here gives the same result).

```{r}
model_parameters(fit_lmm, df_method = "satterthwaite")

t_to_eta2(6.77, df_error = 17)
```

### Bias-Corrected Indices

There are also $\omega_p^2$ (Omega) and $\epsilon^2_p$ (Epsilon), which are less biased versions of $\eta^2_p$ ($\omega^2_p$ is a kin to $R^2_{adj}$; @albers2018power, @mordkoff2019simple).

```{r, eval=TRUE}
F_to_eta2(45.8, 1, 17)
F_to_epsilon2(45.8, 1, 17)
F_to_omega2(45.8, 1, 17)
```


## Measure of Association

Similar to $\eta^2_p$, $r$ (often $r_{pb}$; @cohen1965some) is a signal to noise ratio, and is the squared root of $\eta^2_p$. It is often used instead of $\eta^2_p$ when discussion strength of association (but I suspect people used it instead of $\eta^2_p$ because it gives a bigger number, which looks better).

```{r}
model_parameters(fit_lmm, df_method = "satterthwaite")

t_to_r(6.77, df_error = 17)
```

In a fixed-effect linear model, this returns the **partial** correlation. Compare:

```{r}

fit_lm <- lm(Sepal.Length ~ Sepal.Width + Petal.Length, data = iris) 

model_parameters(fit_lm)

t_to_r(
  t = c(8.59, 27.57), 
  df_error = 147
)

```

to:

```{r, eval=FALSE}
ppcor::pcor(iris[,1:3])$estimate[2:3, 1]
```

```{r, echo=FALSE}
if (require(ppcor, quietly = TRUE)) {
  ppcor::pcor(iris[,1:3])$estimate[2:3, 1]
}
```


## Measures of Difference

These indices represent $Signal/Noise$ with the "signal" representing the difference between two means. This a kin to Cohen's $d$, and is a close approximation when groups are of equal size [@wolf1986meta; @rosnow2000contrasts].

These can be useful approximation in a contrast analysis.

### Between-Subject Contrasts

```{r}
warp.lm <- lm(breaks ~ tension, data = warpbreaks)

pairs(emmeans(warp.lm,  ~ tension))

t_to_d(
  t = c(2.5, 3.7, 1.2),
  df_error = 51
)

```

### Within-Subject Contrasts

```{r}

pairs(emmeans(aov_fit, ~ angle))

t_to_d(
  t = c(-5.7, -5.9, -3.2),
  df_error = 18,
  paired = TRUE
)

```

(Note `paired = TRUE` to not over estimate the size of the effect; @rosenthal1991meta; @rosnow2000contrasts)


# References
